{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Translation Machine - Transformers.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "UBUhGZJCjFVd"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOGp3o06j0a6LYcGLDKIh4I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravishankarramakrishnan/Deep_Learning_Practice/blob/master/Translation_Machine_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss7TSGbLbP3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the Common Libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmyXL9dlbdiq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the Deep Learning Libraries\n",
        "\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds # For Tokenizing the Sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frclxQnhcE9z",
        "colab_type": "code",
        "outputId": "3b70360d-d697-4758-bca7-82845a7d4957",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "# Mount the Drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwPbPN1eeDSj",
        "colab_type": "text"
      },
      "source": [
        "## Importing the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_8GYhLzcR14",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading all the Datasets\n",
        "\n",
        "# Loading the English Dataset\n",
        "\n",
        "with open(\"/content/drive/My Drive/Mordern NLP - Python/Translator using Transformers/europarl-v7.fr-en.en\", \n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "      europarl_en = f.read()\n",
        "\n",
        "# Loading the French Dataset\n",
        "\n",
        "with open(\"/content/drive/My Drive/Mordern NLP - Python/Translator using Transformers/europarl-v7.fr-en.fr\", \n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "      europarl_fr = f.read()\n",
        "\n",
        "# Loading the Non Breaking English Dataset\n",
        "\n",
        "with open(\"/content/drive/My Drive/Mordern NLP - Python/Translator using Transformers/nonbreaking_prefix.en\", \n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "      non_breaking_prefix_en = f.read()\n",
        "\n",
        "# Loading the Non Breaking French Dataset\n",
        "\n",
        "with open(\"/content/drive/My Drive/Mordern NLP - Python/Translator using Transformers/nonbreaking_prefix.fr\", \n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "      non_breaking_prefix_fr = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pt0x6yUdfls",
        "colab_type": "code",
        "outputId": "34633616-5cb2-4470-9193-82bf11aff736",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "# Check for the Dataset\n",
        "\n",
        "print(europarl_en[:50]) # Get first 50 Chars\n",
        "print(europarl_fr[:50]) # Get first 50 Chars\n",
        "print(non_breaking_prefix_en[:5]) # Get first 50 Chars\n",
        "print(non_breaking_prefix_fr[5:10]) # Get first 50 Chars"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Resumption of the session\n",
            "I declare resumed the se\n",
            "Reprise de la session\n",
            "Je déclare reprise la sessio\n",
            "a\n",
            "b\n",
            "c\n",
            "\n",
            "d\n",
            "e\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6hI6QNYeAEb",
        "colab_type": "text"
      },
      "source": [
        "## Cleaning the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t5wTJkqeB6z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get non breaking prefixes as clean list with space and dot at beginning and end as it can ease the Data Cleaning Process\n",
        "\n",
        "# For English\n",
        "# Split the Values by \\n\n",
        "non_breaking_prefix_en = non_breaking_prefix_en.split(\"\\n\")\n",
        "# Add Space adn Dot at beginning and End\n",
        "non_breaking_prefix_en = [' ' + pref + '.' for pref in non_breaking_prefix_en]\n",
        "\n",
        "# For French\n",
        "# Split the Values by \\n\n",
        "non_breaking_prefix_fr = non_breaking_prefix_fr.split(\"\\n\")\n",
        "# Add Space adn Dot at beginning and End\n",
        "non_breaking_prefix_fr = [' ' + pref + '.' for pref in non_breaking_prefix_fr]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSjVd43We7Ch",
        "colab_type": "code",
        "outputId": "7a122efe-dd35-48c8-eb1b-9fc01749c146",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        }
      },
      "source": [
        "# Validation\n",
        "\n",
        "non_breaking_prefix_fr"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' a.',\n",
              " ' b.',\n",
              " ' c.',\n",
              " ' d.',\n",
              " ' e.',\n",
              " ' f.',\n",
              " ' g.',\n",
              " ' h.',\n",
              " ' i.',\n",
              " ' j.',\n",
              " ' k.',\n",
              " ' l.',\n",
              " ' m.',\n",
              " ' n.',\n",
              " ' o.',\n",
              " ' p.',\n",
              " ' q.',\n",
              " ' r.',\n",
              " ' s.',\n",
              " ' t.',\n",
              " ' u.',\n",
              " ' v.',\n",
              " ' w.',\n",
              " ' x.',\n",
              " ' y.',\n",
              " ' z.',\n",
              " ' mme.',\n",
              " ' mlle.',\n",
              " ' c.-à-d.',\n",
              " ' cf.',\n",
              " ' chap.',\n",
              " ' e.g.',\n",
              " ' al.',\n",
              " ' etc.',\n",
              " ' ex.',\n",
              " ' fig.',\n",
              " ' suiv.',\n",
              " ' sup.',\n",
              " ' suppl.',\n",
              " ' tél.',\n",
              " ' vol.',\n",
              " ' vs.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TN1WJ_yfdwQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Do for the Whole Corpus\n",
        "\n",
        "# Tokenize function will do jobs like lowercase, space before, fullstop, comma etc, as the tokenizer function does it\n",
        "# We want to remove the Points that are not full stops\n",
        "# So we need each word and symbol that we want to keep in lowercase and seperated by spaces so we can tokenize them. So each point that is'nt a fullstop will have 3 ### sign\n",
        "# Perform this is non breaking Prefix datafile\n",
        "\n",
        "# Do for English Corpus\n",
        "corpus_en = europarl_en\n",
        "\n",
        "# Replace . with ###\n",
        "for prefix in non_breaking_prefix_en:\n",
        "    corpus_en = corpus_en.replace(prefix, prefix + \"###\")\n",
        "# If we have . followed by any charecters other than spaces, then it isnt considered fullstop and we add ### here\n",
        "corpus_en = re.sub(r\"\\.(?=[0-9]|[A-Z]|[a-z])\", \".###\", corpus_en)\n",
        "corpus_en = re.sub(r\".###\", \" \", corpus_en)\n",
        "# Remove Extra Whitespaces\n",
        "corpus_en = re.sub(r\"  +\", \" \", corpus_en)\n",
        "corpus_en = corpus_en.split(\"\\n\")\n",
        "\n",
        "# Do for French Corpus\n",
        "corpus_fr = europarl_fr\n",
        "\n",
        "# Replace . with ###\n",
        "for prefix in non_breaking_prefix_fr:\n",
        "    corpus_fr = corpus_fr.replace(prefix, prefix + \"###\")\n",
        "# If we have . followed by any charecters other than spaces, then it isnt considered fullstop and we add ### here\n",
        "corpus_fr = re.sub(r\"\\.(?=[0-9]|[A-Z]|[a-z])\", \".###\", corpus_fr)\n",
        "corpus_fr = re.sub(r\".###\", \" \", corpus_fr)\n",
        "# Remove Extra Whitespaces\n",
        "corpus_fr = re.sub(r\"  +\", \" \", corpus_fr)\n",
        "corpus_fr = corpus_fr.split(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dery-YPSiqkz",
        "colab_type": "code",
        "outputId": "9b6ffcb9-fc3a-4c7d-f8ea-c0bd2df8f466",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "print(corpus_en[:5])\n",
        "print(corpus_fr[:5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Resumption of the session', 'I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.', \"Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\", 'You have requested a debate on this subject in the course of the next few days, during this part-session.', \"In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\"]\n",
            "['Reprise de la session', 'Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances.', 'Comme vous avez pu le constater, le grand \"bogue de l\\'an 2000\" ne s\\'est pas produit. En revanche, les citoyens d\\'un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles.', 'Vous avez souhaité un débat à ce sujet dans les prochains jours, au cours de cette période de session.', \"En attendant, je souhaiterais, comme un certain nombre de collègues me l'ont demandé, que nous observions une minute de silence pour toutes les victimes, des tempêtes notamment, dans les différents pays de l'Union européenne qui ont été touchés.\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_as1ZS3myfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenizing the Sentences\n",
        "# Tokenization transforms each charecters into one corresponding number\n",
        "\n",
        "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_en, target_vocab_size = 2**13)\n",
        "\n",
        "tokenizer_fr = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_fr, target_vocab_size = 2**13)\n",
        "\n",
        "# It will take a lot of time to Run"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hUWPpJZkziR",
        "colab_type": "code",
        "outputId": "a5d95178-2cd8-4d7f-8a3c-cd30c67669ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2 # 2 - One before and After a word\n",
        "VOCAB_SIZE_FR = tokenizer_fr.vocab_size + 2 # 2 - One before and After a word\n",
        "print(VOCAB_SIZE_EN, VOCAB_SIZE_FR)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8186 8168\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iY2jZIM9gj6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the Inputs and Outputs from the Datasets - -2 start, -1 end\n",
        "\n",
        "inputs = [ [VOCAB_SIZE_EN - 2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN - 1] for sentence in corpus_en]\n",
        "outputs = [ [VOCAB_SIZE_FR - 2] + tokenizer_fr.encode(sentence) + [VOCAB_SIZE_FR - 1] for sentence in corpus_fr]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yp4w8o5KpDTq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Removing Long Sentences from the Corpus\n",
        "# Issues: When we pad all sentences, we want all sentences in same length for the Batch to work, and it'll take a lot of time to train if very long sentences are taken\n",
        "MAX_LENGTH = 20\n",
        "\n",
        "# Perform Processing for Inputs\n",
        "# To test if len of sentence is too high, we can get the Count of the Index in the very sentence\n",
        "idx_to_remove = [ count for count, sent in enumerate(inputs) if len(sent) > MAX_LENGTH ]\n",
        "\n",
        "# If we want to remove 2 and 4 th element (for elements), the elements may shift to the previous or the next one, so we process in reverse order so we process and remove the right element\n",
        "\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "\n",
        "# Perform Processing for Outputs\n",
        "# To test if len of sentence is too high, we can get the Count of the Index in the very sentence\n",
        "idx_to_remove = [ count for count, sent in enumerate(outputs) if len(sent) > MAX_LENGTH ]\n",
        "\n",
        "# If we want to remove 2 and 4 th element (for elements), the elements may shift to the previous or the next one, so we process in reverse order so we process and remove the right element\n",
        "\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BIi7Q_-rG3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inputs and Outputs Creation\n",
        "\n",
        "# Pad Inputs and Outputs to bring each sentence to the Same Length\n",
        "# Padding - Add 0 at the end(post) of each sentence for Inputs\n",
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                            value=0,\n",
        "                                            padding=\"post\",\n",
        "                                            maxlen = MAX_LENGTH)\n",
        "\n",
        "# Padding - Add 0 at the end(post) of each sentence for Outputs\n",
        "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
        "                                            value=0,\n",
        "                                            padding=\"post\",\n",
        "                                            maxlen = MAX_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jcdh3fIksANJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating our Datasets\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000 # Shuffling of the Datasets\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
        "\n",
        "# To improve the way dataset is stored and have access, increasing the speed of training etc, we use\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "# To Improve sped of training and have access to data faster \n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNpYgecQ1a0u",
        "colab_type": "text"
      },
      "source": [
        "## Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_58VDPB1iYd",
        "colab_type": "text"
      },
      "source": [
        "![Transformer Architecture](https://i0.wp.com/blog.exxactcorp.com/wp-content/uploads/2019/05/1_blSbN23mOGMZ_DWvTAcO1w.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gx-96K7J1zn8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First Step: Creating the Embeddings and the Positional Encoding. We use Position Embeddings sin and cos as defined in the paper of the Transformer\n",
        "\n",
        "# Positional Encoding Layer\n",
        "\n",
        "class PositionalEncoding(layers.Layer):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "    # Peform Embedding Based on Formula, sin for even and cos for odd\n",
        "\n",
        "    def get_angles(self, pos, i, d_model): # Shape should be pos: (seq_length, 1)., i: (1, d_model)\n",
        "        angles = 1/np.power(10000., (2 ** (i // 2)) / np.float(d_model))\n",
        "        return pos * angles # To get shape as (seq_length, d_model)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        seq_length = inputs.shape.as_list()[-2]\n",
        "        d_model = inputs.shape.as_list()[-1]\n",
        "\n",
        "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis], # Creating seq_length, 1\n",
        "                                 np.arange(d_model)[np.newaxis, :],    # Creating 1, d_model\n",
        "                                 d_model)\n",
        "        \n",
        "        # For Even\n",
        "        angles[:, 0::2] = np.sin(angles[:, 0::2]) # Take all columns, Start from Row 0 with Stride of 2\n",
        "        # For Odd\n",
        "        angles[:, 1::2] = np.cos(angles[:, 1::2]) # Take all columns, Start from Row 1 with Stride of 2\n",
        "\n",
        "        pos_encoding = angles[np.new_axis, ...] # Add New Axis and ... means Keep Everything Else\n",
        "        return inputs + tf.cast(pos_encoding, tf.float_32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rvg6a4Uc6lT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Scaled Dot Product Attention - Attention Mechanism\n",
        "\n",
        "# Attention Computation: Attention(Q, k, v) = softmax(Q * k.T/ sqrt(d.k)) * v\n",
        "\n",
        "# Function for Scaled Dot Product\n",
        "\n",
        "def scaled_dot_product_attention(queries, keys, values, mask): # Mask is Lookahead Mask that doesnt allow decoder to see next word or padding mask that pads 0 at end of each sentence\n",
        "    product = tf.matmul(queries, keys, transpose_b= True) # transpose_b shows the 2nd matrix (keys) to be transposed\n",
        "    # Get the Values of dK\n",
        "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float_32) # Get all dimensions of the keys and take the Last one as it corresponds to Embedding\n",
        "\n",
        "    # Get Scaled Dot Product\n",
        "    scaled_product = product / tf.math.sqrt(keys_dim)\n",
        "\n",
        "    # Add the Mask (May be Optional As per Paper)\n",
        "    if mask is not None:\n",
        "        # Multiply by -infinity as we are going to apply Softmax next. Softmax gives values between 0 and 1, and the sum of all equals 1\n",
        "        scaled_product += (mask * -1e9)  # We take 1e9 as close to infinity, as softmax takes the 0's as very low weightage\n",
        "    \n",
        "    # Get the Attention Mechanism Formula\n",
        "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis = -1), values) # Axis= -1 is to be applied on last axis., softmax(Q * k.T/ sqrt(d.k)) - gives weightage, so we apply softmax to make it to 1\n",
        "\n",
        "    return attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muN-Ua4I-3vk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Multi Headed Attention SubLayer\n",
        "\n",
        "# Linear Function is dense layer in tf, split result into sub space and apply scaled dot product\n",
        "\n",
        "class MultiHeadAttention(layers.Layer): # It inherits from layers.Layer\n",
        "\n",
        "    def __init(self, nb_proj):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.nb_proj = nb_proj\n",
        "\n",
        "    # Define Build, Similar to init, here if we use the Object for the First time, Unlike init (called when object is created), It is called the first time the object is used\n",
        "    def build(self, input_shape): # Input Shape will exactly be the Shape of Queries\n",
        "        self.d_model = input_shape[-1] # Last Dimension of the Inputs\n",
        "        # Dimension of Each Projection = dim of model / nb of projection\n",
        "        # Make Sure to when dividing as integers, keep it and others will be 0. Assert demands to have this Condition\n",
        "        assert self.d_model % self.nb_proj == 0\n",
        "\n",
        "        self.d_proj = self.d_model // self.nb_proj\n",
        "\n",
        "        self.query_lin = layers.Dense(units= self.d_model)\n",
        "        self.key_lin = layers.Dense(units= self.d_model)\n",
        "        self.value_lin = layers.Dense(units= self.d_model)\n",
        "        self.final_lin = layers.Dense(units= self.d_model)\n",
        "\n",
        "    # Write Function for split_proj\n",
        "    def split_proj(self, inputs, batch_size): # Batch size is used to reshape the Inputs so it is splitted into subspaces, Shape will be inputs: (batch_size, seq_length, d_model)\n",
        "        shape = (batch_size, -1, self.nb_proj, self.d_proj)\n",
        "        splited_inputs = tf.reshape(inputs, shape = shape) # The Shape will be (batch_size, seq_length, nb_proj, d_proj)\n",
        "        # We need to Alter the Outputs to get result shape as (batch_size, nb_proj, seq_length, d_proj)\n",
        "        return tf.transpose(splited_inputs, perm= [0,2,1,3])\n",
        "    \n",
        "    # Writing Architecture of the Sub Layer\n",
        "    def call(self, queries, keys, values, mask):\n",
        "\n",
        "        batch_size = tf.shape(queries)[0]\n",
        "\n",
        "        # Apply Huge Linear Function to them\n",
        "        queries = self.query_lin(queries)\n",
        "        keys = self.key_lin(keys)\n",
        "        values = self.value_lin(values)\n",
        "        # Query, Key, Value.. lin are 3 Dense Layers that we declare now\n",
        "\n",
        "        # Split the Inputs after applying the Linear Function - To Subspaces\n",
        "        queries = self.split_proj(queries, batch_size)\n",
        "        keys = self.split_proj(keys, batch_size)\n",
        "        values = self.split_proj(values, batch_size)\n",
        "\n",
        "        # Compute the Attention for Each of those SubSpaces\n",
        "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
        "\n",
        "        # Perform Concatenation - Opposite of Split_proj, precisely the splited_inputs method. We invert the Dimension once again\n",
        "        attention = tf.transpose(attention, perm= [0,2,1,3])\n",
        "\n",
        "        concat_attention = tf.reshape(attention, shape= (batch_size, -1, self.d_model))\n",
        "\n",
        "        outputs  = self.final_lin(concat_attention)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8OgXQhcNs4M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build the Encoder\n",
        "\n",
        "class EncoderLayer(layers.Layer):\n",
        "\n",
        "    def __init__(self, FFN_units, nb_proj, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        self.d_model = input_shape[-1]\n",
        "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate = self.dropout)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon= 1e-6)\n",
        "        self.dense_1 = layers.Dense(units= self.FFN_units, activation= \"relu\")\n",
        "        self.dense_2 = layers.Dense(units= self.d_model, activation= \"relu\")\n",
        "        self.dropout_2 = layers.Dropout(rate = self.dropout)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon= 1e-6)\n",
        "\n",
        "\n",
        "    # We use 3 inputs here because 3 (Q, K, V) are same, so we use 3 inputs\n",
        "    def call(self, inputs, mask, training):\n",
        "        attention = self.multi_head_attention(inputs,\n",
        "                                              inputs,\n",
        "                                              inputs,\n",
        "                                              mask)\n",
        "        attention = self.dropout_1(attention, training= training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "\n",
        "        # Create Feed Forward Part\n",
        "        outputs = self.dense_1(attention)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_2(outputs)\n",
        "        outputs = self.norm_2(outputs + attention)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_olXfYgxcrYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Whole Encoder\n",
        "\n",
        "class Encoder(layers.Layer):\n",
        "\n",
        "    def __init__(self, nb_layers, FFN_units, nb_proj, dropout, vocab_size, d_model, name=\"encoder\"):\n",
        "        super(Encoder, self).__init__(name= name)\n",
        "        self.nb_layers = nb_layers\n",
        "        #self.FFN_units = FFN_units\n",
        "        #self.nb_proj = nb_proj\n",
        "        #self.dropout = dropout\n",
        "        self.d_model = d_model\n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate = dropout)\n",
        "        self.enc_layers = [EncoderLayer(FFN_units,\n",
        "                                        nb_proj,\n",
        "                                        dropout)\n",
        "                          for _ in range(nb_layers)]\n",
        "\n",
        "    def call(self, inputs, mask, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)) # As defined in Paper\n",
        "        outputs = self.pos_encoding(outputs) # It returns the Sum of the Outputs and the Positional Encoding\n",
        "        outputs = self.dropout(output, training)\n",
        "\n",
        "        # Get Layers as list\n",
        "        for i in range(self.nb_layers):\n",
        "            outputs = self.enc_layers[i](outputs, mask, training)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5fg7DYjiAWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build the Decoder\n",
        "\n",
        "# Create the Decoder Sub Layer\n",
        "\n",
        "class DecoderLayer(layers.Layer):\n",
        "\n",
        "    def __init__(self, FFN_units, nb_proj, dropout): # nb_proj - Used for Multihead Attention Sublayer; FFN_units - Feed Forward Network\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def build(self, input_shape): # Shape of Input\n",
        "        self.d_model = input_shape[-1] # To get the Last Dimension\n",
        "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate= self.dropouts)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon= 1e-6)\n",
        "\n",
        "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_2 = layers.Dropout(rate= self.dropouts)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon= 1e-6)\n",
        "\n",
        "        self.dense_1 = layers.Dense(units= self.FFN_units,\n",
        "                                    activation= \"relu\")\n",
        "        self.dense_2 = layers.Dense(units= self.d_model)\n",
        "        \n",
        "        self.dropout_3 = layers.Dropout(rate= self.dropout)\n",
        "        self.norm_3 = layers.LayerNormalization(epsilon= 1e-6)\n",
        "\n",
        "    # Layer that uses Multihead Inputs after Positional Encoding\n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training): # 2 Masks for Self Attention Layer and Attention Layer which uses outputs of the Encoder\n",
        "        attention = self.multi_head_attention_1(inputs, inputs, inputs, mask_1) # Q,K,V are same, so we use 3 inputs here\n",
        "        attention = self.dropout_1(attention, training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "\n",
        "        # Attention Layer that uses the Output of the Encoder\n",
        "        attention_2 = self.multi_head_attention_2(attention, enc_outputs, enc_outputs, mask_2) # Q - attention, K,V - Output of Encoder, as defined in the Paper\n",
        "        attention_2 = self.dropout_2(attention_2, training) # Training is Boolean, whether True or False\n",
        "        attention_2 = self.norm_2(attention_2 + attention)\n",
        "\n",
        "        # Encoder Layer, A Simple Feed Forward Sequence\n",
        "\n",
        "        outputs = self.dense_1(attention_2)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_3(outputs, training)\n",
        "        outputs = self.norm_3(outputs + attention_2)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7pQECr-FK29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build the Decoder\n",
        "\n",
        "class Decoder(layers.Layer):\n",
        "\n",
        "    def __init__(self, nb_layers, FFN_units, nb_proj, dropout, vocab_size, d_model, name=\"decoder\"):\n",
        "        super(Decoder, self).__init__(name= name)\n",
        "        self.d_model = d_model\n",
        "        self.nb_layers = nb_layers\n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate= dropout)\n",
        "\n",
        "        self.dec_layers = [ DecoderLayer(FFN_units,\n",
        "                                         nb_proj,\n",
        "                                         dropout) for _ in range(nb_layers) ]\n",
        "\n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "\n",
        "        # Write Code for Decoder to till Multiheaded attention, taken care by decoding layer\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs += tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training)\n",
        "\n",
        "        # Apply Multiple Decoder Sublayers\n",
        "\n",
        "        for i in range(self.nb_layers):\n",
        "            outputs = self.decoder_layers[i](outputs,\n",
        "                                             enc_outputs,\n",
        "                                             mask_1,\n",
        "                                             mask_2,\n",
        "                                             training)\n",
        "\n",
        "        return outpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20mX6Yz1IoQV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Building the TransFormer\n",
        "\n",
        "# Create 2 Mask tokens, First to mask padding tokens at <EOS> and other to create Lookahead Mask, that makes sure decoder has access to Future Masks\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, vocab_size_enc, vocab_size_dec, d_model, nb_layers, FFN_units, nb_proj, dropout, name= \"transformer\"): # 2 Embeddings, One for Encoder and other for Decoder\n",
        "        super(Transformer, self).__init__(name= name)\n",
        "\n",
        "        self.encoder = Encoder(nb_layers, FFN_units, nb_proj, dropout, vocab_size_enc, d_model)\n",
        "        self.decoder = Decoder(nb_layers, FFN_units, nb_proj, dropout, vocab_size_dec, d_model)\n",
        "        self.last_linear = layers.Dense(units= vocab_size_dec) # Size is same as decoder vocab as we want to get the Probablity of Each possible word in our Vocab to be the Next one\n",
        "\n",
        "    # Build 2 Masking Functions\n",
        "    def create_padding_mask(self, seq): # Shape of seq is (batch_size, seq_length)\n",
        "        # Look for 0's in the Padding token\n",
        "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32) # Represent it as float, Cast - Casts a tensor to a new datatype.\n",
        "        return mask[:, tf.newaxis, tf.newaxis, :] # This Mask is applied to Attention Computation & Multihead Attention Sublayer, Before Scaled Dot Product Attention. Add 2 New Dimension\n",
        "        # Before Applying scaled dotproduct attention, we split data via split proj. So that is the Reason for Adding new Axis. Thanks to Empty Dimension (NewAxis), the Row will be repeated as many times as we need\n",
        "\n",
        "        # Check on for Matrix Multiplication - Create Matrix that cant have access to Future words, So we use Triangular Matrix\n",
        "        def create_look_ahead_mask(self, seq):\n",
        "            seq_len = tf.shape(seq)[1] # Get the Shape of Matrix and 1st element\n",
        "            look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0) # Create a Triangular Matrix and add 0 to top right part of the Matrix. -1 says we dont want to use this one and 0 says we want j to be >= i\n",
        "            return look_ahead_mask\n",
        "\n",
        "        # Combine the Encoder and Decoder Architecture\n",
        "\n",
        "        def call(self, enc_inputs, dec_inputs, training):\n",
        "            # Add Just Simple Padding Mask for the Encoder\n",
        "            enc_mask = self.create_padding_mask(enc_inputs)\n",
        "            # Add Lookahead Mask for the Decoder - see example for Clarity\n",
        "            dec_mask_1 = tf.maximum(self.create_padding_mask(dec_inputs),\n",
        "                                  self.create_look_ahead_mask(dec_inputs))\n",
        "            # Add 2nd Mask of the Decoder\n",
        "            dec_mask_2 = self.create_padding_mask(enc_inputs) # This is for 2nd Multihead mask where we apply mask for encoder and decoder inputs\n",
        "            # This is for Rearrangement of Outputs of Encoder, following the way Sentences from the Decoder are related to sentence from the Decoder\n",
        "            # The Final Shape of the Multihead Attention Layer will have the Same Shape as what we have inside the Decoder, and way we acheive this is by combining the Information from the Encoder and decoder (Matrix Multiplication)\n",
        "\n",
        "            enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
        "            dec_outputs = self.decoder(dec_inputs, enc_outputs, dec_mask_1, dec_mask_2, training)\n",
        "\n",
        "            outputs = self.last_linear(dec_outputs)\n",
        "\n",
        "            return outputs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjRjtS7qjAXm",
        "colab_type": "text"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXhMtXPfjDfk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clear all the Sessions\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Define the Hyperparameters - After Comments # is what was defined on the Paper of Transformers\n",
        "D_MODEL = 128 # 512\n",
        "NB_LAYERS = 4 # 6\n",
        "FFN_UNITS = 512 # 2048\n",
        "NB_PROJ = 8 # 8\n",
        "DROPOUT = 0.1 # 0.1\n",
        "\n",
        "# Building the Transformer\n",
        "\n",
        "transformer = Transformer(vocab_size_enc= VOCAB_SIZE_EN,\n",
        "                          vocab_size_dec = VOCAB_SIZE_FR,\n",
        "                          d_model = D_MODEL,\n",
        "                          nb_layers = NB_LAYERS,\n",
        "                          FFN_units = FFN_UNITS,\n",
        "                          nb_proj = NB_PROJ,\n",
        "                          dropout = DROPOUT)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEMpDSdC23i8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build Functions Related to Loss and Accuracy\n",
        "# We use Categorical and get Probablities as to which word will be the next one (inside our Vocabulary)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits= True, # OP of Model are real nos that becomes Probablity in future by applying Softmax\n",
        "                                                            reduction= \"none\") # We Dont want one Loss number to be there for the Whole Batch\n",
        "\n",
        "def loss_function(target, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(target, 0)) # Get rid of All the Losses that Corresponds to 0 in Targets\n",
        "    loss_ = loss_object(target, pred)\n",
        "    # Multiply Mask to Loss\n",
        "\n",
        "    # Make Mask Same Type\n",
        "    mask = tf.cast(mask, dtype= loss_.dtype) # Now Values inside loss and mask will be of Same type\n",
        "    # Multiply Loss with Mask - for putting 0's on the Padding Tokens So, we only use Projections for Real sentence and not just 0's in the End\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_) # return what we get after having sum over all Batches/Dimensions\n",
        "\n",
        "# Create Train Loss that Keeps track of Losses during training\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name= \"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name= \"train_accuracy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIXrvauz6xyQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# As given in paper, we use the Optimizer to indicate on how to use the Gradients and Optimize model\n",
        "\n",
        "# Create Custom Learning Rate\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    # To Create Own Custom Learning Rate\n",
        "\n",
        "    def __init__(self, d_model, warmup_steps= 4000): # 4000 as defined in the Paper\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model= tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    # Write Call Function at initialisation, as we dont need to add any more Layers\n",
        "    def __call__(self, step): # Step is always the Parameter in Learning Rate Scheduler. It'll deal by itself\n",
        "        # Implement as shown in Paper\n",
        "        arg1 = tf.math.rsqrt(step) # Reciprocal of Sqrt Elementwise => Power^-0.5\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkgQNQ9dbyKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now Create a Custom Learning Rate\n",
        "\n",
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "# Create the Optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate= learning_rate,\n",
        "                                     beta_1= 0.9,\n",
        "                                     beta_2= 0.98,\n",
        "                                     epsilon= 1e-9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqlJTllucoZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create CheckPoints and Store the Model at the End of each Epoch\n",
        "\n",
        "checkpoint_path = \"./drive/My Drive/Mordern NLP - Python/Translator using Transformers/ckpt/\"\n",
        "\n",
        "# Create Checkpoint Object - To Store the State of Optimizer and Transformer\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer= transformer,\n",
        "                           optimizer= optimizer)\n",
        "# Create Checkpoint Manager\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep= 5) # Store 5 Different Checkpoints\n",
        "\n",
        "# Check if Checkpoint Already exists\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest Checkpoint Restored\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qp4iRBVgUsb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Loop to Train the Model\n",
        "\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"Start of Epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "\n",
        "    # Clean History Of Losses and Accuracy at the Beginning of an Epoch\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "\n",
        "    # Iterate over Each Batch\n",
        "    for (batch, (enc_inputs, targets)) in enumerate(dataset): # Our Dataset Contains input and Output, so we get it as Tuple\n",
        "        dec_inputs = targets[:, :-1] # Target - last words\n",
        "        dec_outputs_real = targets[:, 1:] # Get after the First Word\n",
        "        # Apply the Transformer\n",
        "        with tf.GradientTape() as tape: # Tape Records everything that happens while prediction\n",
        "            predictions = transformer(enc_inputs, dec_inputs, True) # True for Training\n",
        "            loss = loss_function(dec_outputs_real, predictions)\n",
        "\n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "        # Add to Train Loss\n",
        "        train_loss(loss)\n",
        "        train_accuracy(dec_outputs_real, predictions)\n",
        "\n",
        "        # Pint Loss and Accuracy from Time to Time\n",
        "\n",
        "        if batch % 50 == 0:\n",
        "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(epoch+1,\n",
        "                                                                         batch,\n",
        "                                                                         train_loss.result(),\n",
        "                                                                         train_accuracy.result()))\n",
        "        # At End of Epoch, Save the Model to Checkpoint\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print(\"Saving Checkpoint for Epoch {} at {}\".format(epoch+1, ckpt_save_path))\n",
        "\n",
        "        print(\"Time Taken for 1 Epoch is: {} secs \\n\".format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKDn3erdl138",
        "colab_type": "text"
      },
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpLK6U30l3fQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the Input from User and Encode the Values\n",
        "\n",
        "def evaluate(inp_sentence):\n",
        "    # Add Starting and Ending Token and Pad the Words\n",
        "    inp_sentence = [VOCAB_SIZE_EN - 2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN - 1] # -2 for Starting Token, -1 for Ending Token\n",
        "    # Simulate Batch Dimension\n",
        "    enc_input = tf.expand_dims(inp_sentence, axis= 0) # Axis 0 Corresponds to the Batch\n",
        "\n",
        "    output = tf.expand_dims([VOCAB_SIZE_FR - 2], axis= 0) # Simulate for Batch\n",
        "\n",
        "    # Create Several Iteration of Transformer\n",
        "\n",
        "    for _ in range(MAX_LENGTH):\n",
        "        predictions = transformer(enc_input, output, False) # If Training is False, Dropout is not Applied. Dimension 1 - for Batch Size. Dimension => (1, seq_length, vocab_size_fr)\n",
        "\n",
        "        prediction = predictions[:, -1:, :]\n",
        "\n",
        "        # Get Index of Next word - Higher the probablity, higher the number is next\n",
        "        predicted_id = tf.cast(tf.argmax(prediction, axis= -1), tf.int32) # Apply along the Last axis\n",
        "        \n",
        "        # Test for Ending token\n",
        "        if predicted_id == VOCAB_SIZE_FR - 1:\n",
        "            # Translation is Done\n",
        "            return tf.squeeze(output, axis= 0) # Gets rid of 1st Dimension\n",
        "\n",
        "        # Add the New Index to the Output\n",
        "        output = tf.concat([output, predicted_id], axis= -1) # Output is Sequence of Numbers representing the Sentence\n",
        "    \n",
        "    return tf.squeeze(output, axis= 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7rX_ycbpoGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a Function Translate that takes input\n",
        "\n",
        "def translate(sentence):\n",
        "    output = evaluate(sentence).numpy() # This is an Encoded Version of the Output\n",
        "\n",
        "    # Decode the Encoded Output - Dont Decode the Starting Token\n",
        "    predicted_sentence = tokenizer_fr.decode(\n",
        "        [ i for i in output if i < VOCAB_SIZE_FR - 2] ) # We Kept Only the numbers that corresponds to Real Words\n",
        "\n",
        "    print(\"User Input: {}\".format(sentence))\n",
        "    print(\"Predicted Translation: {}\".format(predicted_sentence))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en2YLhP_q4Hk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To Translate\n",
        "\n",
        "translate(\"Enter the Sentence\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBUhGZJCjFVd",
        "colab_type": "text"
      },
      "source": [
        "## Model Sample Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kpqs6Ir1cQ9",
        "colab_type": "code",
        "outputId": "e4159409-b3b9-434a-d1c7-da5dee2e0e72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        }
      },
      "source": [
        "# Sample for Transformer Model\n",
        "\n",
        "# Sample for Creation of Triangular Matrix. This is how we can say that words i(row) will not have access to words j(column). So we want to Mask the words that appears in the Future\n",
        "print(tf.ones((5, 5)))\n",
        "print(tf.linalg.band_part(tf.ones((5, 5)), -1, 0))\n",
        "print(1 - tf.linalg.band_part(tf.ones((5, 5)), -1, 0))\n",
        "\n",
        "# Sample 2 For Understanding the Masks\n",
        "\n",
        "def create_padding_mask(seq): # Shape of seq is (batch_size, seq_length)\n",
        "    # Look for 0's in the Padding token\n",
        "    mask = tf.cast(tf.math.equal(seq, 0), tf.float32) # Represent it as float, Cast - Casts a tensor to a new datatype.\n",
        "    return mask[:, tf.newaxis, tf.newaxis, :] # This Mask is applied to Attention Computation & Multihead Attention Sublayer, Before Scaled Dot Product Attention. Add 2 New Dimension\n",
        "    # Before Applying scaled dotproduct attention, we split data via split proj. So that is the Reason for Adding new Axis. Thanks to Empty Dimension (NewAxis), the Row will be repeated as many times as we need\n",
        "\n",
        "# Check on for Matrix Multiplication - Create Matrix that cant have access to Future words, So we use Triangular Matrix\n",
        "def create_look_ahead_mask(seq):\n",
        "    seq_len = tf.shape(seq)[1] # Get the Shape of Matrix and 1st element\n",
        "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0) # Create a Triangular Matrix and add 0 to top right part of the Matrix. -1 says we dont want to use this one and 0 says we want j to be >= i\n",
        "    return look_ahead_mask\n",
        "\n",
        "print(\"\\n After Defenition, See about Masks and data sample \\n\")\n",
        "\n",
        "# Create a Fake Sequence\n",
        "seq = tf.cast([[837, 835, 0, 273, 9, 0, 0, 0]], tf.int32)\n",
        "print(\"Sequence is \", seq, \".See 0's in data are completely masked as 1\")\n",
        "\n",
        "# When we apply Self Attention Mechanism, at beginning of Decoder, we use lookahead mask so we dont have access to Future words, but also the Padding mask\n",
        "\n",
        "tf.maximum(create_padding_mask(seq), create_look_ahead_mask(seq))\n",
        "# We get shape as (1, 1, 8, 8), which tells about (batch_size, nb_proj, size_of_matrix, size_of_matrix) - size_of_matrix for Attention weights. 1 says we apply mask(1 corresponds to dont take / 0, see seq matrix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1.]], shape=(5, 5), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[1. 0. 0. 0. 0.]\n",
            " [1. 1. 0. 0. 0.]\n",
            " [1. 1. 1. 0. 0.]\n",
            " [1. 1. 1. 1. 0.]\n",
            " [1. 1. 1. 1. 1.]], shape=(5, 5), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[0. 1. 1. 1. 1.]\n",
            " [0. 0. 1. 1. 1.]\n",
            " [0. 0. 0. 1. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0.]], shape=(5, 5), dtype=float32)\n",
            "\n",
            " After Defenition, See about Masks and data sample \n",
            "\n",
            "Sequence is  tf.Tensor([[837 835   0 273   9   0   0   0]], shape=(1, 8), dtype=int32) .See 0's in data are completely masked as 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 1, 8, 8), dtype=float32, numpy=\n",
              "array([[[[0., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [0., 0., 1., 1., 1., 1., 1., 1.],\n",
              "         [0., 0., 1., 1., 1., 1., 1., 1.],\n",
              "         [0., 0., 1., 0., 1., 1., 1., 1.],\n",
              "         [0., 0., 1., 0., 0., 1., 1., 1.],\n",
              "         [0., 0., 1., 0., 0., 1., 1., 1.],\n",
              "         [0., 0., 1., 0., 0., 1., 1., 1.],\n",
              "         [0., 0., 1., 0., 0., 1., 1., 1.]]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xO6_DgcaLYK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHJiq9EuFFDs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "time.sleep(5000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_7CDxN0FNLC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}